{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrEhWG7hQFz2"
   },
   "source": [
    "# Speech Recognition with AI\n",
    "Brought to you by Daniel Sikar - daniel.sikar@city.ac.uk\n",
    "and\n",
    "City Data Science Society - https://www.datasciencesociety.city/\n",
    "\n",
    "## Natural Language Processing with Convolutional Neural Networks\n",
    "\n",
    "Notebook: https://github.com/dsikar/natural-language-processing/blob/master/NaturalLanguageProcessing.ipynb\n",
    "\n",
    "Tensorflow's Speech Commands Datasets: http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
    "\n",
    "Consisting of:\n",
    "* 65,000 one-second long utterances\n",
    "* 30 short words plus a background noise set\n",
    "* Thousands of different people\n",
    "\n",
    "Using a subset (\"yes\" and \"no\") of Tensorflow's Speech Commands Datasets. The full set consists of 30 words plus a background noise set: _background_noise_, bed, bird, cat, dog, down, eight, five, four, go, happy, house, left, marvin, no, nine, off, on, one, right, seven, sheila, six, stop, three, tree, two, up, wow, yes, zero.\n",
    "\n",
    "Note: In this workshop, we will **not** use the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LO0n_MKgKuAd",
    "outputId": "1d92ba73-4bab-4945-aaa3-81e760ae3ba3"
   },
   "outputs": [],
   "source": [
    "# Get the data subset\n",
    "# Install PyDrive\n",
    "!pip install PyDrive\n",
    "\n",
    "#Import modules\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "#Authenticate and create the PyDrive client\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Get the shareable link e.g. https://drive.google.com/file/d/1OebaOg7YlGHa1UYQDIpPOuNWNTn2AHlq/view?usp=sharing\n",
    "# Get the id from the link 1OebaOg7YlGHa1UYQDIpPOuNWNTn2AHlq\n",
    "downloaded = drive.CreateFile({'id':\"1OebaOg7YlGHa1UYQDIpPOuNWNTn2AHlq\"})   \n",
    "downloaded.GetContentFile('nlp-dataset.tar.gz')   \n",
    "# Alternatively, if you are running the notebook locally, file can be downloaded by pasting shareable link\n",
    "# into browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8U6RT8FNRku8"
   },
   "outputs": [],
   "source": [
    "# list\n",
    "# !ls\n",
    "# unpack\n",
    "# !tar xvf nlp-dataset.tar.gz\n",
    "# !ls dataset/training\n",
    "# !ls dataset/training/no\n",
    "# !ls dataset/training/yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-8k0KFHVDU_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UF3pcKTLY4F4",
    "outputId": "c6cac18b-3b28-42cc-8fda-9575007f0147"
   },
   "outputs": [],
   "source": [
    "# what is the space available?\n",
    "!df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koNkhOEeVDVD"
   },
   "source": [
    "**Data Exploration and Visualization**\n",
    "\n",
    "Data Exploration and Visualization helps us to understand the data as well as pre-processing steps in a better way. \n",
    "\n",
    "**Visualization of Audio signal in time series domain**\n",
    "\n",
    "Now, we’ll visualize the audio signal in the time series domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "PYAne6bsVDVE",
    "outputId": "1febbc87-14e2-4d97-f3e8-c29f0164ff8a"
   },
   "outputs": [],
   "source": [
    "# about 20 Hz to 20 kHz\n",
    "# Humans can detect sounds in a frequency range from about 20 Hz to 20 kHz.\n",
    "train_audio_path = 'dataset/training/'\n",
    "samples, sample_rate = librosa.load(train_audio_path + 'yes/8d8d9855_nohash_0.wav', sr = 8000)\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('\"Yes\" - waveform for file ' + 'dataset/training/yes/8d8d9855_nohash_0.wav')\n",
    "ax1.set_xlabel('time')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.plot(np.linspace(0, sample_rate/len(samples), sample_rate), samples)\n",
    "ipd.Audio(samples, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "yksflJfHM20A",
    "outputId": "8b3e22e6-4f0b-40b1-9e7a-62924897f5cf"
   },
   "outputs": [],
   "source": [
    "train_audio_path = 'dataset/training/'\n",
    "samples, sample_rate = librosa.load(train_audio_path + 'no/8a194ee6_nohash_0.wav', sr = 8000)\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('\"NO\" - Waveform for file ' + 'dataset/training//no/8a194ee6_nohash_0.wav')\n",
    "ax1.set_xlabel('time')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.plot(np.linspace(0, sample_rate/len(samples), sample_rate), samples)\n",
    "ipd.Audio(samples, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TekJpdHVDVH",
    "outputId": "be34bb3b-8b48-4dbf-f12b-e88426bcce5a"
   },
   "outputs": [],
   "source": [
    "print(type(samples))\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Y-mmIx7VDVJ"
   },
   "outputs": [],
   "source": [
    "# Human voice: \n",
    "# In telephony, the usable voice frequency band ranges from approximately 300 to 3400 Hz\n",
    "# The bandwidth allocated for a single voice-frequency transmission channel is usually 4 kHz\n",
    "# Per the Nyquist–Shannon sampling theorem, the sampling frequency (8 kHz) must be at least twice \n",
    "# the highest component of the voice frequency via appropriate filtering prior to sampling at discrete \n",
    "# times (4 kHz) for effective reconstruction of the voice signal. \n",
    "\n",
    "# TODO plot frequency x ampliture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4FepQniVDVK",
    "outputId": "5f63489c-7a77-4315-8190-02a6e0fcfd91"
   },
   "outputs": [],
   "source": [
    "labels=os.listdir(train_audio_path)\n",
    "print(\"Audio labels: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "c-v8oU4_VDVL",
    "outputId": "630b3a63-75a2-4126-d744-1f666b53ef71"
   },
   "outputs": [],
   "source": [
    "#find count of each label and plot bar graph\n",
    "no_of_recordings=[]\n",
    "for label in labels:\n",
    "    waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')]\n",
    "    no_of_recordings.append(len(waves))\n",
    "    \n",
    "#plot\n",
    "plt.figure()\n",
    "index = np.arange(len(labels))\n",
    "plt.bar(index, no_of_recordings)\n",
    "plt.xlabel('Commands', fontsize=12)\n",
    "plt.ylabel('No of recordings', fontsize=12)\n",
    "plt.xticks(index, labels, fontsize=15, rotation=60)\n",
    "plt.title('No. of recordings for each command')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GXI7vAUVDVM"
   },
   "source": [
    "**Duration of recordings**\n",
    "\n",
    "What’s next? A look at the distribution of the duration of recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "uOs6bFxoVDVM",
    "outputId": "62d21ebe-5a53-4748-eb45-58588eeeda1a"
   },
   "outputs": [],
   "source": [
    "duration_of_recordings=[]\n",
    "for label in labels:\n",
    "    waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')]\n",
    "    for wav in waves:\n",
    "        sample_rate, samples = wavfile.read(train_audio_path + '/' + label + '/' + wav)\n",
    "        duration_of_recordings.append(float(len(samples)/sample_rate))\n",
    "    \n",
    "plt.hist(np.array(duration_of_recordings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCteLkKjVDVN"
   },
   "source": [
    "**Preprocessing the audio waves**\n",
    "\n",
    "In the data exploration part earlier, we have seen that the duration of a few recordings is less than 1 second and the sampling rate is too high. So, let us read the audio waves and use the below-preprocessing steps to deal with this.\n",
    "\n",
    "Here are the two steps we’ll follow:\n",
    "\n",
    "* Resampling\n",
    "* Removing shorter commands of less than 1 second\n",
    "\n",
    "Let us define these preprocessing steps in the below code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwZQZb2OVDVN",
    "outputId": "051dc828-4ee2-4b6d-836a-0f6a6e649995"
   },
   "outputs": [],
   "source": [
    "# 743s execution time\n",
    "train_audio_path = 'dataset/training'\n",
    "\n",
    "all_wave = []\n",
    "all_label = []\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')]\n",
    "    for wav in waves:\n",
    "        samples, sample_rate = librosa.load(train_audio_path + '/' + label + '/' + wav, sr = 8000)\n",
    "        #samples = librosa.resample(samples, sample_rate, 8000)\n",
    "        if(len(samples)== 8000) : \n",
    "            all_wave.append(samples)\n",
    "            all_label.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wccoYDmVDVO"
   },
   "source": [
    "Convert the output labels to integer encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Il8BsTOxVDVO",
    "outputId": "e7eb61b2-6387-499c-a7ef-920d40cd7993"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y=le.fit_transform(all_label)\n",
    "classes= list(le.classes_)\n",
    "type(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLkG3xoHVDVP"
   },
   "source": [
    "Now, convert the integer encoded labels to a one-hot vector since it is a multi-classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKz8IkKoVDVP"
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y=np_utils.to_categorical(y, num_classes=len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJnHIuQjVDVP"
   },
   "source": [
    "Reshape the 2D array to 3D since the input to the conv1d must be a 3D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4J9_wofFVDVQ"
   },
   "outputs": [],
   "source": [
    "all_wave = np.array(all_wave).reshape(-1,8000,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhKW07cqVDVQ"
   },
   "source": [
    "**Split into train and validation set**\n",
    "\n",
    "Next, we will train the model on 80% of the data and validate on the remaining 20%:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMOgC_cYVDVR"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(np.array(all_wave),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bqEi398VDVR"
   },
   "source": [
    "**Model Architecture for this problem**\n",
    "\n",
    "We will build the speech-to-text model using conv1d. Conv1d is a convolutional neural network which performs the convolution along only one dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z93-K5A4VDVR"
   },
   "source": [
    "**Model building**\n",
    "\n",
    "Let us implement the model using Keras functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rByRV5zmVDVR",
    "outputId": "bc8f8576-06f9-4a50-e7ae-70e1b4bf44a4"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "inputs = Input(shape=(8000,1))\n",
    "\n",
    "#First Conv1D layer\n",
    "conv = Conv1D(8,13, padding='valid', activation='relu', strides=1)(inputs)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Second Conv1D layer\n",
    "conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Third Conv1D layer\n",
    "conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Fourth Conv1D layer\n",
    "conv = Conv1D(64, 7, padding='valid', activation='relu', strides=1)(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Flatten layer\n",
    "conv = Flatten()(conv)\n",
    "\n",
    "#Dense Layer 1\n",
    "conv = Dense(256, activation='relu')(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Dense Layer 2\n",
    "conv = Dense(128, activation='relu')(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "outputs = Dense(len(labels), activation='softmax')(conv)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Rbfe9b0VDVS"
   },
   "source": [
    "Define the loss function to be categorical cross-entropy since it is a multi-classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JO7e5ktVDVS"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ry3Vx_mVDVT"
   },
   "source": [
    "Early stopping and model checkpoints are the callbacks to stop training the neural network at the right time and to save the best model after every epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKy96wwLVDVT"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, min_delta=0.0001) \n",
    "mc = ModelCheckpoint('best_model.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZFVT1MNVDVT"
   },
   "source": [
    "Let us train the model on a batch size of 32 and evaluate the performance on the holdout set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTxvY0SRVDVT",
    "outputId": "51e4b263-c9ce-44ee-c96c-165b1747a060"
   },
   "outputs": [],
   "source": [
    "# execution time 258s\n",
    "history=model.fit(x_tr, y_tr ,epochs=10, callbacks=[es,mc], batch_size=32, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('nlp-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify \n",
    "!ls -lh nlp-model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmFKmJMBVDVU"
   },
   "source": [
    "**Diagnostic plot**\n",
    "\n",
    "I’m going to lean on visualization again to understand the performance of the model over a period of time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "m59CuQffVDVU",
    "outputId": "ceb31709-5af0-47b3-bdde-a5e6cbac6c57"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train_loss')\n",
    "pyplot.plot(history.history['val_loss'], label='test_loss')\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2wwJyKcVDVU"
   },
   "source": [
    "**Loading the best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "bTFy_piaVDVV",
    "outputId": "f05274a3-7751-442d-b608-36babc1eb7cd"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model('nlp-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBc6L2bsVDVV"
   },
   "source": [
    "Define the function that predicts text for the given audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJsFQBK3VDVV"
   },
   "outputs": [],
   "source": [
    "def predict(audio):\n",
    "    prob=model.predict(audio.reshape(1,8000,1))\n",
    "    index=np.argmax(prob[0])\n",
    "    return prob, classes[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foZlaOhcVDVV"
   },
   "source": [
    "Prediction time! Make predictions on the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "i1AxW5OWVDVX",
    "outputId": "f00d6df2-f8a7-4cf2-e0ed-e336b5401e9b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "print(\"len(x_val):\", len(x_val))\n",
    "index=random.randint(0,len(x_val)-1)\n",
    "print(\"index:\", index)\n",
    "samples=x_val[index].ravel() # x_val[index] shape: (8000, 1), \"samples\" shape: (8000,)\n",
    "print(\"x_val[index]:\", x_val[index])\n",
    "print(\"Audio:\",classes[np.argmax(y_val[index])])\n",
    "ipd.Audio(samples, rate=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_X2VM9RA6zM",
    "outputId": "4d35b2de-e7ae-4564-d5fa-4bf0f65ca07d"
   },
   "outputs": [],
   "source": [
    "# x_val[index].shape\n",
    "# (8000, 1)\n",
    "# x_val[index]: x_val[index]: [[ 0.00422602]\n",
    "# [ 0.01268432]\n",
    "# [ 0.00283716]\n",
    "# ...\n",
    "# xr = x_val[index].ravel()\n",
    "# type(xr) # numpy.ndarray\n",
    "# type(x_val[index]) # numpy.ndarray\n",
    "# xr.shape # (8000,)\n",
    "# xr: array([-0.00036546, -0.00062576,  0.00048751, ..., -0.0003528 ,\n",
    "y_val[index]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NaturalLanguageProcessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
